{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bengo\\funny_folder\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW   \n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "# from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 650000/650000 [00:01<00:00, 614866.46 examples/s]\n",
      "Generating test split: 100%|██████████| 50000/50000 [00:00<00:00, 634840.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\", cache_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 8094.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Llama-encoder-1.0B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train = small_train_dataset.map(tokenize_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenized_train.remove_columns([\"text\"])\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at Llama-encoder-1.0B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 110])\n",
      "attention_mask torch.Size([1, 110])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/3000 [00:08<7:11:25,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 90])\n",
      "attention_mask torch.Size([1, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/3000 [00:13<5:19:01,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 152])\n",
      "attention_mask torch.Size([1, 152])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/3000 [00:19<5:06:11,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 206])\n",
      "attention_mask torch.Size([1, 206])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/3000 [00:26<5:25:24,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 501])\n",
      "attention_mask torch.Size([1, 501])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/3000 [00:40<7:40:53,  9.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 240])\n",
      "attention_mask torch.Size([1, 240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/3000 [00:48<7:23:38,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 62])\n",
      "attention_mask torch.Size([1, 62])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/3000 [00:53<6:16:03,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 476])\n",
      "attention_mask torch.Size([1, 476])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/3000 [01:06<7:39:16,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 398])\n",
      "attention_mask torch.Size([1, 398])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/3000 [01:17<8:07:03,  9.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 411])\n",
      "attention_mask torch.Size([1, 411])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/3000 [01:28<8:32:14, 10.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 70])\n",
      "attention_mask torch.Size([1, 70])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/3000 [01:33<7:15:29,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 366])\n",
      "attention_mask torch.Size([1, 366])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/3000 [01:45<8:00:37,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 29])\n",
      "attention_mask torch.Size([1, 29])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/3000 [01:50<6:42:27,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 157])\n",
      "attention_mask torch.Size([1, 157])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/3000 [01:57<6:32:45,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 229])\n",
      "attention_mask torch.Size([1, 229])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/3000 [02:06<6:52:36,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 66])\n",
      "attention_mask torch.Size([1, 66])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/3000 [02:11<6:06:09,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 117])\n",
      "attention_mask torch.Size([1, 117])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/3000 [02:18<5:49:34,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 162])\n",
      "attention_mask torch.Size([1, 162])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/3000 [02:25<5:50:14,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 473])\n",
      "attention_mask torch.Size([1, 473])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 19/3000 [02:39<7:43:49,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 573])\n",
      "attention_mask torch.Size([1, 573])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/3000 [02:57<9:45:37, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 64])\n",
      "attention_mask torch.Size([1, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/3000 [03:02<8:10:51,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 75])\n",
      "attention_mask torch.Size([1, 75])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/3000 [03:08<7:03:48,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 20])\n",
      "attention_mask torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 23/3000 [03:12<6:00:04,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 172])\n",
      "attention_mask torch.Size([1, 172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24/3000 [03:20<6:07:04,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 353])\n",
      "attention_mask torch.Size([1, 353])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 25/3000 [03:32<7:12:58,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 27])\n",
      "attention_mask torch.Size([1, 27])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/3000 [03:36<6:11:51,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 124])\n",
      "attention_mask torch.Size([1, 124])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 27/3000 [03:43<6:01:16,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 377])\n",
      "attention_mask torch.Size([1, 377])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 28/3000 [03:56<7:31:35,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([1])\n",
      "input_ids torch.Size([1, 529])\n",
      "attention_mask torch.Size([1, 529])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(tokenized_train, shuffle=True, batch_size=1)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Llama-encoder-1.0B\", num_labels=5)\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "# model.model.gradient_checkpointing = True\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        preprocess_batch = {}\n",
    "        for k, v in batch.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                preprocess_batch[k] = v.to(device)\n",
    "            elif isinstance(v, list):\n",
    "                preprocess_batch[k] = torch.stack(v, dim=1).to(device)\n",
    "\n",
    "        for k, v in preprocess_batch.items():\n",
    "            print(k,v.shape)\n",
    "        # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**preprocess_batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
